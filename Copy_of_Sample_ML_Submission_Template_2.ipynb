{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name** - PhonePe Transaction Insights\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Data Analysis and Visualization Project with Business Intelligence (BI) Use Cases\n",
        "##### **Contribution**    - Individual\n",
        "##### **Name** - Shreya Saha\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The PhonePe Transaction Insights project is an end-to-end data analytics and visualization initiative focused on analyzing digital payment data from PhonePe, one of India‚Äôs largest financial technology platforms. With the growing reliance on digital payments, understanding user behavior, transaction trends, and regional payment patterns is critical for strategic decision-making in financial services. This project aims to transform raw transaction data into meaningful business intelligence through data extraction, SQL-based analysis, Python visualizations, and an interactive Streamlit dashboard."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/ShreyaSaha012005/PhonePe-Transaction-Insights"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the rapid growth of digital payment platforms in India, understanding user behavior and transaction trends is critical for enhancing financial services, ensuring security, and improving customer experience. PhonePe, being one of the leading digital payment applications, generates massive volumes of transaction data across different states, districts, and pin codes.\n",
        "\n",
        "However, this data remains largely unstructured and underutilized unless properly extracted, analyzed, and visualized. There is a pressing need to develop an analytical system that can process this data to provide meaningful insights into user engagement, payment patterns, insurance usage, and geographical trends.\n",
        "\n",
        "This project aims to bridge that gap by leveraging data engineering, SQL analysis, and interactive visualizations to:\n",
        "\n",
        "Identify top-performing regions and user segments,\n",
        "\n",
        "Analyze the popularity of payment categories and insurance products,\n",
        "\n",
        "Detect behavioral trends and seasonal patterns in transactions,\n",
        "\n",
        "Provide a dashboard-based interface for intuitive business decision-making.\n",
        "\n",
        "By doing so, the project supports data-driven strategy formulation in areas like customer segmentation, fraud detection, marketing optimization, and product development for digital financial platforms."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing essential libraries for the PhonePe Transaction Insights project\n",
        "\n",
        "# File and data handling\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# SQL and database interaction\n",
        "import sqlite3  # or use sqlalchemy/pymysql for MySQL/PostgreSQL\n",
        "\n",
        "# Data visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "\n",
        "# Streamlit (for dashboard ‚Äì used locally or via Jupyter Streamlit magic)\n",
        "# You won't use Streamlit directly in Colab, but useful in the repo\n",
        "try:\n",
        "    import streamlit as st\n",
        "except:\n",
        "    pass  # Ignore if Streamlit not available in this environment\n",
        "\n",
        "# Utility\n",
        "from datetime import datetime\n"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Extract the zip\n",
        "with zipfile.ZipFile(\"pulse.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"pulse\")\n",
        "\n",
        "# Verify extraction\n",
        "print(\"Folders inside extracted pulse directory:\")\n",
        "print(os.listdir(\"pulse\"))\n"
      ],
      "metadata": {
        "id": "IAvP77cwr8iP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# Define correct base path\n",
        "base_path = \"/content/pulse/pulse/data/aggregated/transaction/country/india/state\"\n",
        "\n",
        "# List available states\n",
        "states = os.listdir(base_path)\n",
        "print(f\"Total states found: {len(states)}\")\n",
        "print(\"Sample states:\", states[:5])\n",
        "\n",
        "# Define a sample JSON file path\n",
        "sample_state = \"andaman-&-nicobar-islands\"\n",
        "sample_year = \"2018\"\n",
        "sample_quarter = \"1\"\n",
        "\n",
        "sample_file_path = os.path.join(base_path, sample_state, sample_year, f\"{sample_quarter}.json\")\n",
        "\n",
        "# Load and inspect sample file\n",
        "with open(sample_file_path, 'r') as f:\n",
        "    sample_data = json.load(f)\n",
        "\n",
        "# Show top-level keys\n",
        "print(\"\\nTop-level keys:\", list(sample_data.keys()))\n",
        "\n",
        "# Display transaction data section\n",
        "print(\"\\nSample transaction data:\")\n",
        "for txn in sample_data['data']['transactionData']:\n",
        "    print(f\"Type: {txn['name']}, Count: {txn['paymentInstruments'][0]['count']}, \"\n",
        "          f\"Amount: ‚Çπ{txn['paymentInstruments'][0]['amount']}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# Base directory (update if needed)\n",
        "base_path = \"/content/pulse/pulse/data/aggregated/transaction/country/india/state\"\n",
        "\n",
        "# List of all rows to build\n",
        "data_rows = []\n",
        "\n",
        "# Loop through all states\n",
        "for state in os.listdir(base_path):\n",
        "    state_path = os.path.join(base_path, state)\n",
        "\n",
        "    # Loop through years (e.g., 2018‚Äì2023)\n",
        "    for year in os.listdir(state_path):\n",
        "        year_path = os.path.join(state_path, year)\n",
        "\n",
        "        # Loop through quarters (1.json to 4.json)\n",
        "        for quarter_file in os.listdir(year_path):\n",
        "            if quarter_file.endswith(\".json\"):\n",
        "                quarter_path = os.path.join(year_path, quarter_file)\n",
        "\n",
        "                try:\n",
        "                    with open(quarter_path, 'r') as file:\n",
        "                        content = json.load(file)\n",
        "\n",
        "                        for record in content['data']['transactionData']:\n",
        "                            txn_type = record['name']\n",
        "                            count = record['paymentInstruments'][0]['count']\n",
        "                            amount = record['paymentInstruments'][0]['amount']\n",
        "                            data_rows.append({\n",
        "                                \"State\": state,\n",
        "                                \"Year\": int(year),\n",
        "                                \"Quarter\": int(quarter_file.replace('.json', '')),\n",
        "                                \"Transaction Type\": txn_type,\n",
        "                                \"Count\": count,\n",
        "                                \"Amount\": amount\n",
        "                            })\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing {quarter_path}: {e}\")\n",
        "\n",
        "# Convert to DataFrame\n",
        "df_transactions = pd.DataFrame(data_rows)\n",
        "\n",
        "# Show DataFrame shape\n",
        "print(f\"\\n‚úÖ Dataset Loaded!\")\n",
        "print(f\"Total Rows: {df_transactions.shape[0]}\")\n",
        "print(f\"Total Columns: {df_transactions.shape[1]}\")\n",
        "print(\"\\nColumns:\", list(df_transactions.columns))\n"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display basic info of the dataset\n",
        "print(\"üìò Dataset Info:\\n\")\n",
        "df_transactions.info()\n",
        "\n",
        "# Display first few rows\n",
        "print(\"\\nüîé Sample Rows:\\n\")\n",
        "print(df_transactions.head())\n",
        "\n",
        "# Check for missing values\n",
        "print(\"\\n‚ùì Missing Values:\\n\")\n",
        "print(df_transactions.isnull().sum())\n",
        "\n",
        "# Check data types\n",
        "print(\"\\nüìê Data Types:\\n\")\n",
        "print(df_transactions.dtypes)\n",
        "\n",
        "# Basic descriptive statistics for numeric columns\n",
        "print(\"\\nüìä Descriptive Statistics:\\n\")\n",
        "print(df_transactions.describe())\n"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for exact duplicate rows in the dataset\n",
        "duplicate_count = df_transactions.duplicated().sum()\n",
        "\n",
        "print(f\"üîÅ Total Duplicate Rows: {duplicate_count}\")\n",
        "\n",
        "# Optional: Display duplicate rows if needed\n",
        "if duplicate_count > 0:\n",
        "    print(\"\\nüîé Duplicate Records Preview:\\n\")\n",
        "    display(df_transactions[df_transactions.duplicated()])\n"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing (null) values in each column\n",
        "missing_values = df_transactions.isnull().sum()\n",
        "\n",
        "print(\"üö´ Missing / Null Values Count:\\n\")\n",
        "print(missing_values)\n",
        "\n",
        "# Optional: Show percentage of missing values\n",
        "print(\"\\nüìä Missing Value Percentage:\\n\")\n",
        "print((missing_values / len(df_transactions)) * 100)\n"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set plot style\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# üî• Heatmap of missing values\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.heatmap(df_transactions.isnull(),\n",
        "            cbar=False,\n",
        "            cmap=\"Reds\",\n",
        "            yticklabels=False,\n",
        "            linewidths=0.5)\n",
        "plt.title(\" Heatmap of Missing Values\", fontsize=14)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After conducting an initial analysis of the PhonePe transactions dataset, I discovered the following key points:\n",
        "\n",
        "Dataset Size & Structure:\n",
        "\n",
        "The dataset contains thousands of rows spanning multiple states, years, and quarters.\n",
        "\n",
        "It includes 6 primary columns: State, Year, Quarter, Transaction Type, Count, and Amount.\n",
        "\n",
        "No Missing or Duplicate Data (or minimal):\n",
        "\n",
        "There were no major missing values or duplicates, indicating well-curated data.\n",
        "\n",
        "The schema is clean and consistent, suitable for time series and regional analysis.\n",
        "\n",
        "Hierarchical Data Source:\n",
        "\n",
        "Data is organized in a nested directory format by state ‚Üí year ‚Üí quarter, with each file representing a single time slice in JSON format.\n",
        "\n",
        "Each JSON file contains aggregated transaction data by type (e.g., Recharge, Peer-to-peer payments, etc.).\n",
        "\n",
        "Transaction Types:\n",
        "\n",
        "There are multiple transaction categories, and each includes a count and monetary amount, enabling both frequency and value analysis.\n",
        "\n",
        "Wide Geographic Coverage:\n",
        "\n",
        "The dataset spans all Indian states and union territories, making it useful for geo-level analysis and visualizations (e.g., top performing regions).\n",
        "\n",
        "Time Period:\n",
        "\n",
        "The dataset spans from 2018 to the most recent quarter available in the Pulse GitHub repo, allowing for trend analysis across years and quarters."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the column names of the dataset\n",
        "print(\"üßæ Dataset Columns:\\n\")\n",
        "print(df_transactions.columns.tolist())\n",
        "\n",
        "# Optional: Show with data types\n",
        "print(\"\\nüìê Column Names with Data Types:\\n\")\n",
        "print(df_transactions.dtypes)\n"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show statistical summary for numeric columns\n",
        "print(\"üìä Descriptive Statistics for Numeric Columns:\\n\")\n",
        "print(df_transactions.describe())\n",
        "\n",
        "# Optional: Summary for all columns (including categorical)\n",
        "print(\"\\nüìù Summary for All Columns:\\n\")\n",
        "print(df_transactions.describe(include='all'))\n"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "State:\n",
        "Represents the name of the Indian state or union territory where the transactions were recorded.\n",
        "\n",
        "Year:\n",
        "Indicates the year in which the transaction data was collected (e.g., 2018, 2019, ..., 2023).\n",
        "\n",
        "Quarter:\n",
        "Refers to the quarter of the year during which the transactions took place:\n",
        "\n",
        "Q1 = January‚ÄìMarch\n",
        "\n",
        "Q2 = April‚ÄìJune\n",
        "\n",
        "Q3 = July‚ÄìSeptember\n",
        "\n",
        "Q4 = October‚ÄìDecember\n",
        "\n",
        "Transaction Type:\n",
        "The category or nature of the digital transaction. Common types include:\n",
        "\n",
        "Recharge & bill payments\n",
        "\n",
        "Peer-to-peer payments\n",
        "\n",
        "Merchant payments\n",
        "\n",
        "Financial services\n",
        "\n",
        "Others\n",
        "\n",
        "Count:\n",
        "The total number of transactions for the given type, state, year, and quarter.\n",
        "\n",
        "Amount:\n",
        "The total monetary value (in Indian Rupees ‚Çπ) of the transactions for the given type, state, year, and quarter."
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop through each column and print number of unique values\n",
        "print(\" Unique Value Count Per Column:\\n\")\n",
        "for col in df_transactions.columns:\n",
        "    unique_vals = df_transactions[col].nunique()\n",
        "    print(f\"{col}: {unique_vals} unique values\")\n",
        "\n",
        "# Optional: Show sample unique values for each column\n",
        "print(\"\\nüßæ Sample Unique Values:\\n\")\n",
        "for col in df_transactions.columns:\n",
        "    print(f\"\\n{col} (sample):\")\n",
        "    print(df_transactions[col].unique()[:5])\n"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üõ†Ô∏è Step 1: Standardize Column Names\n",
        "df_transactions.columns = df_transactions.columns.str.strip().str.lower().str.replace(' ', '_')\n",
        "\n",
        "# üõ†Ô∏è Step 2: Strip extra spaces from string columns\n",
        "str_cols = df_transactions.select_dtypes(include='object').columns\n",
        "df_transactions[str_cols] = df_transactions[str_cols].apply(lambda x: x.str.strip())\n",
        "\n",
        "# üõ†Ô∏è Step 3: Convert data types explicitly\n",
        "df_transactions['year'] = df_transactions['year'].astype(int)\n",
        "df_transactions['quarter'] = df_transactions['quarter'].astype(int)\n",
        "df_transactions['count'] = df_transactions['count'].astype(int)\n",
        "df_transactions['amount'] = df_transactions['amount'].astype(float)\n",
        "\n",
        "# üõ†Ô∏è Step 4: Handle duplicates\n",
        "initial_shape = df_transactions.shape\n",
        "df_transactions = df_transactions.drop_duplicates()\n",
        "print(f\"‚úÖ Removed {initial_shape[0] - df_transactions.shape[0]} duplicate rows.\")\n",
        "\n",
        "# üõ†Ô∏è Step 5: Check and handle missing values\n",
        "missing = df_transactions.isnull().sum()\n",
        "if missing.sum() == 0:\n",
        "    print(\"‚úÖ No missing values found.\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Missing values detected:\\n\", missing)\n",
        "    # Optional: Fill or drop depending on your use-case\n",
        "    # df_transactions.fillna(0, inplace=True)\n",
        "\n",
        "# üõ†Ô∏è Step 6: Final overview\n",
        "print(\"\\nüì¶ Final Cleaned Dataset Info:\\n\")\n",
        "df_transactions.info()\n"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To prepare the dataset for analysis, the following data cleaning and transformation steps were applied:\n",
        "\n",
        "Column Name Standardization\n",
        "\n",
        "Converted all column names to lowercase and replaced spaces with underscores for consistency.\n",
        "\n",
        "Whitespace Removal\n",
        "\n",
        "Stripped leading/trailing spaces from all string-based columns like state and transaction_type.\n",
        "\n",
        "Data Type Conversions\n",
        "\n",
        "Explicitly converted:\n",
        "\n",
        "year and quarter to int\n",
        "\n",
        "count to int\n",
        "\n",
        "amount to float\n",
        "\n",
        "Duplicate Removal\n",
        "\n",
        "Identified and removed exact duplicate rows to avoid double-counting.\n",
        "\n",
        "Missing Value Check\n",
        "\n",
        "Verified that there were no missing values in any columns (or handled them if present).\n",
        "\n",
        "Data Flattening\n",
        "\n",
        "Parsed multiple nested JSON files and transformed them into a unified flat table for easier querying and visualization.\n",
        "\n",
        "üìä Insights Found from the Dataset\n",
        "Based on the initial exploration, here are some key insights:\n",
        "\n",
        "Transaction Distribution by Type\n",
        "\n",
        "Categories like Peer-to-Peer Payments and Recharge & Bill Payments have the highest transaction counts and amounts across most states.\n",
        "\n",
        "Top Performing States\n",
        "\n",
        "States like Maharashtra, Karnataka, and Uttar Pradesh consistently show high transaction volumes and values, indicating high digital payment adoption.\n",
        "\n",
        "Year-over-Year Growth\n",
        "\n",
        "A clear increase in transaction volume and value is observed from 2018 to 2022, reflecting the growing trend in digital payments.\n",
        "\n",
        "Quarterly Trends\n",
        "\n",
        "Q4 (Oct‚ÄìDec) quarters tend to show spikes in transactions, possibly due to festive season spending.\n",
        "\n",
        "Low Activity Regions\n",
        "\n",
        "Union Territories like Lakshadweep and Andaman & Nicobar Islands have significantly lower transaction figures.\n",
        "\n",
        "High Value, Low Frequency Categories\n",
        "\n",
        "Some transaction types have fewer counts but high total amounts, indicating use of digital payments for large-value services (e.g., Financial Services or Merchant Payments in some regions).\n"
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SQL Backend Tables and Queries:**\n",
        "\n",
        "The SQL database is organized into three categories: Aggregated Tables, Map Tables, and Top Tables, each storing different types of PhonePe-like data.\n",
        "\n",
        "The Aggregated Tables (Aggregated_user, Aggregated_transaction, Aggregated_insurance) store state-wise summary information. For example, Aggregated_user tracks how many users registered and opened the app in each state per quarter, while Aggregated_transaction contains data on the number and value of transactions by type (like recharge, P2P, merchant). Similarly, Aggregated_insurance holds data on insurance policies issued and claimed amounts over time.\n",
        "\n",
        "The Map Tables (Map_user, Map_map, Map_insurance) provide district-level data for each category. These are useful for analyzing user behavior, transaction volume, or insurance activity geographically within a state.\n",
        "\n",
        "The Top Tables (Top_user, Top_map, Top_insurance) highlight the highest-performing districts, pin codes, or insurance categories in each quarter. These tables are designed to help identify top users or regions in terms of engagement or transaction value.\n",
        "\n",
        "Altogether, this structure enables detailed analysis of trends, user activity, financial transactions, and insurance data over time and across locations, helping generate meaningful business insights.\n"
      ],
      "metadata": {
        "id": "BhGV8vd0XTpy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "\n",
        "# Create a connection to a new SQLite database in memory (or save to file with 'pulse.db')\n",
        "conn = sqlite3.connect('pulse.db')\n",
        "cursor = conn.cursor()\n",
        "# Aggregated Tables\n",
        "cursor.execute(\"\"\"\n",
        "CREATE TABLE Aggregated_user (\n",
        "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "    state TEXT,\n",
        "    year INTEGER,\n",
        "    quarter INTEGER,\n",
        "    registered_users INTEGER,\n",
        "    app_opens INTEGER\n",
        ")\n",
        "\"\"\")\n",
        "\n",
        "cursor.execute(\"\"\"\n",
        "CREATE TABLE Aggregated_transaction (\n",
        "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "    state TEXT,\n",
        "    year INTEGER,\n",
        "    quarter INTEGER,\n",
        "    transaction_type TEXT,\n",
        "    transaction_count INTEGER,\n",
        "    transaction_amount REAL\n",
        ")\n",
        "\"\"\")\n",
        "\n",
        "cursor.execute(\"\"\"\n",
        "CREATE TABLE Aggregated_insurance (\n",
        "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "    state TEXT,\n",
        "    year INTEGER,\n",
        "    quarter INTEGER,\n",
        "    insurance_type TEXT,\n",
        "    policies_issued INTEGER,\n",
        "    claim_amount REAL\n",
        ")\n",
        "\"\"\")\n",
        "\n",
        "# Map Tables\n",
        "cursor.execute(\"\"\"\n",
        "CREATE TABLE Map_user (\n",
        "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "    state TEXT,\n",
        "    district TEXT,\n",
        "    year INTEGER,\n",
        "    quarter INTEGER,\n",
        "    registered_users INTEGER\n",
        ")\n",
        "\"\"\")\n",
        "\n",
        "cursor.execute(\"\"\"\n",
        "CREATE TABLE Map_map (\n",
        "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "    state TEXT,\n",
        "    district TEXT,\n",
        "    year INTEGER,\n",
        "    quarter INTEGER,\n",
        "    transaction_amount REAL\n",
        ")\n",
        "\"\"\")\n",
        "\n",
        "cursor.execute(\"\"\"\n",
        "CREATE TABLE Map_insurance (\n",
        "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "    state TEXT,\n",
        "    district TEXT,\n",
        "    year INTEGER,\n",
        "    quarter INTEGER,\n",
        "    policies_issued INTEGER\n",
        ")\n",
        "\"\"\")\n",
        "\n",
        "# Top Tables\n",
        "cursor.execute(\"\"\"\n",
        "CREATE TABLE Top_user (\n",
        "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "    state TEXT,\n",
        "    year INTEGER,\n",
        "    quarter INTEGER,\n",
        "    district TEXT,\n",
        "    registered_users INTEGER\n",
        ")\n",
        "\"\"\")\n",
        "\n",
        "cursor.execute(\"\"\"\n",
        "CREATE TABLE Top_map (\n",
        "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "    state TEXT,\n",
        "    year INTEGER,\n",
        "    quarter INTEGER,\n",
        "    location_type TEXT,  -- state, district, pin_code\n",
        "    location_value TEXT,\n",
        "    transaction_amount REAL\n",
        ")\n",
        "\"\"\")\n",
        "\n",
        "cursor.execute(\"\"\"\n",
        "CREATE TABLE Top_insurance (\n",
        "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "    state TEXT,\n",
        "    year INTEGER,\n",
        "    quarter INTEGER,\n",
        "    insurance_category TEXT,\n",
        "    total_policies INTEGER\n",
        ")\n",
        "\"\"\")\n",
        "\n",
        "conn.commit()\n",
        "print(\"‚úÖ All tables created successfully.\")\n"
      ],
      "metadata": {
        "id": "Q8bBmOHNWXiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(data=df_transactions, x='state', y='count', estimator=sum, ci=None, palette='viridis')\n",
        "plt.title(\"Total Transactions by State\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.ylabel(\"Total Transaction Count\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A bar plot is ideal for comparing categorical variables like states with total transaction counts."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "States like Karnataka and Uttar Pradesh show significantly higher transaction volumes compared to others."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Business Impact:\n",
        "These insights help identify high-performing regions for strategic investment, marketing, or infrastructure scaling.\n",
        "\n",
        "Regions with lower activity like Delhi or Tamil Nadu may need user engagement or awareness campaigns.\n",
        "\n",
        "Negative Growth Insight:\n",
        "If a major state like Delhi shows consistently lower counts despite being urban, it could indicate platform under-penetration, suggesting a lost business opportunity in an otherwise promising market."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set the figure size\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot total transaction amount per state\n",
        "sns.barplot(\n",
        "    data=df_transactions,\n",
        "    x='state',\n",
        "    y='amount',\n",
        "    estimator=sum,\n",
        "    ci=None,\n",
        "    palette='coolwarm'\n",
        ")\n",
        "\n",
        "# Customize the plot\n",
        "plt.title(\"Total Transaction Amount by State\", fontsize=14)\n",
        "plt.ylabel(\"Total Amount (‚Çπ)\", fontsize=12)\n",
        "plt.xlabel(\"State\", fontsize=12)\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A bar plot again suits this purpose well to compare monetary values (‚Çπ) across states.\n",
        "\n",
        "It complements Chart 1 by showing not just activity (count) but value contribution."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Karnataka and Uttar Pradesh not only have higher transaction counts but also the highest total transaction value.\n",
        "\n",
        "Delhi, despite being an urban hub, shows a relatively lower monetary contribution."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Business Impact:\n",
        "These insights highlight where the most valuable users are, which helps businesses focus premium services, insurance products, or high-value campaigns in these states.\n",
        "\n",
        "Negative Growth Insight:\n",
        "States with lower total value despite moderate counts (like Delhi) could imply low-value transactions or poor service penetration in high-income areas, needing investigation."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Step 1: Create a new column if not already present\n",
        "df_transactions['avg_transaction_value'] = df_transactions['amount'] / df_transactions['count']\n",
        "\n",
        "# Step 2: Plot average transaction value by state\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(\n",
        "    data=df_transactions,\n",
        "    x='state',\n",
        "    y='avg_transaction_value',\n",
        "    estimator=np.mean,\n",
        "    ci=None,\n",
        "    palette='cubehelix'\n",
        ")\n",
        "plt.title(\"Average Transaction Value by State\", fontsize=14)\n",
        "plt.ylabel(\"Avg Transaction Value (‚Çπ)\", fontsize=12)\n",
        "plt.xlabel(\"State\", fontsize=12)\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This bar chart shows the average monetary value of transactions, helping compare states on a ‚Äúquality per transaction‚Äù basis rather than volume."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A state might have fewer transactions but a higher average value, which could indicate wealthier users or high-value service use (e.g., insurance, financial services).\n",
        "\n",
        "For instance, Delhi or Maharashtra may show a higher average per transaction even if total counts are moderate."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Yes. It helps:\n",
        "\n",
        "Target premium services to states with high average transaction values.\n",
        "\n",
        "Understand where micro-transactions dominate (low average value = low-margin business).\n",
        "\n",
        "Any negative growth insight?\n",
        "A decline in average transaction value in an otherwise high-volume state may indicate:\n",
        "\n",
        "Users are only using basic services.\n",
        "\n",
        "There's a lack of trust or awareness in higher-value features like insurance or credit."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot: Total Transactions by Year\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.barplot(data=df_transactions, x='year', y='count', estimator=sum, ci=None, palette='Blues')\n",
        "plt.title(\"Total Transactions per Year\", fontsize=14)\n",
        "plt.ylabel(\"Total Transaction Count\", fontsize=12)\n",
        "plt.xlabel(\"Year\", fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A year-wise bar chart is ideal for visualizing how transactions have changed over time."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can observe trends such as steady growth or drops in user engagement. As it can be observed that it has only been increased over the years."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Will it help business impact?\n",
        "Yes. It allows teams to:\n",
        "\n",
        "Forecast transaction growth.\n",
        "\n",
        "Identify periods of seasonal high/low activity.\n",
        "\n",
        "Align product launches and marketing with growth trends.\n",
        "\n",
        "Negative growth insight?\n",
        "Any drop in year-on-year transactions (e.g., 2020) could indicate:\n",
        "\n",
        "External events (e.g., lockdowns)\n",
        "\n",
        "Technical/service issues\n",
        "\n",
        "Increased competition\n",
        "\n"
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot: Total Transaction Amount per Year\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.barplot(data=df_transactions, x='year', y='amount', estimator=sum, ci=None, palette='Greens')\n",
        "plt.title(\"Total Transaction Amount per Year\", fontsize=14)\n",
        "plt.ylabel(\"Total Transaction Amount (‚Çπ)\", fontsize=12)\n",
        "plt.xlabel(\"Year\", fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This helps identify growth trends in financial value, even if transaction volume remained steady.\n",
        "\n",
        "Very useful to see the economic scale of user activity."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This helps identify growth trends in financial value, even if transaction volume remained steady.\n",
        "\n",
        "Very useful to see the economic scale of user activity."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Will it help business impact?\n",
        "\n",
        "Yes, it:\n",
        "\n",
        "Helps prioritize years where performance spiked.\n",
        "\n",
        "Informs financial forecasting and investment planning.\n",
        "\n",
        "Negative growth insight?\n",
        "If a drop is seen in total amount despite high transaction counts, it could mean:\n",
        "\n",
        "Users are only making low-value payments.\n",
        "\n",
        "There's a lack of uptake in high-value features (e.g., insurance, loans).\n",
        "\n"
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot: Total Transaction Count by Transaction Type\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(data=df_transactions, x='transaction_type', y='count', estimator=sum, ci=None, palette='Set2')\n",
        "plt.title(\"Total Transaction Count by Type\", fontsize=14)\n",
        "plt.ylabel(\"Total Transaction Count\", fontsize=12)\n",
        "plt.xlabel(\"Transaction Type\", fontsize=12)\n",
        "plt.xticks(rotation=30)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A categorical bar plot is ideal for comparing different types of user behavior (e.g., bill payments vs. merchant payments).\n",
        "\n",
        "Helps understand what users do the most on the platform."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Peer-to-peer payments, merchant payments and recharge & bill payments dominate transaction counts.\n",
        "\n",
        "Financial Services and Others are used less frequently."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Will it help business impact?\n",
        "Yes. It enables:\n",
        "\n",
        "Prioritizing platform features that drive engagement.\n",
        "\n",
        "Identifying low-performing categories for improvement, incentives, or educational outreach.\n",
        "\n",
        "Negative growth insight?\n",
        "If useful but high-revenue categories (e.g., Financial Services) show very low usage:\n",
        "\n",
        "It points to low user awareness, complexity, or lack of trust.\n",
        "\n",
        "This is a growth bottleneck and an opportunity for product improvement or better UI/UX design."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Plot: Total Transaction Amount by Transaction Type\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(data=df_transactions, x='transaction_type', y='amount', estimator=sum, ci=None, palette='Set1')\n",
        "plt.title(\"Total Transaction Amount by Type\", fontsize=14)\n",
        "plt.ylabel(\"Total Amount (‚Çπ)\", fontsize=12)\n",
        "plt.xlabel(\"Transaction Type\", fontsize=12)\n",
        "plt.xticks(rotation=30)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This complements Chart 6. A category with fewer transactions might still dominate in value.\n",
        "\n",
        "It helps uncover high-value business segments."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Categories like Financial Services do not lead in volume and also they do not contribute significantly to the overall value.\n",
        "\n",
        "Peer to peer Payments may show both high volume and high value."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Will it help business impact?\n",
        "Definitely. It shows:\n",
        "\n",
        "Where the big money flows.\n",
        "\n",
        "Which categories are strategic revenue drivers and should be expanded, advertised, or optimized.\n",
        "\n",
        "Negative growth insight?\n",
        "If important categories like insurance, loans, or merchant services show low value:\n",
        "\n",
        "This could suggest customer mistrust, lack of product-market fit, or competition pulling users away.\n",
        "\n"
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Plot: Total Transaction Count by Quarter\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.barplot(data=df_transactions, x='quarter', y='count', estimator=sum, ci=None, palette='Oranges')\n",
        "plt.title(\"Total Transactions per Quarter\", fontsize=14)\n",
        "plt.xlabel(\"Quarter\", fontsize=12)\n",
        "plt.ylabel(\"Total Transaction Count\", fontsize=12)\n",
        "plt.xticks([0, 1, 2, 3], [\"Q1\", \"Q2\", \"Q3\", \"Q4\"])\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It‚Äôs essential for spotting seasonal effects‚Äîe.g., higher digital payments during the festive quarter (Q4)."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Often, Q4 (Oct‚ÄìDec) has higher transaction volumes due to:\n",
        "\n",
        "Diwali/festive shopping\n",
        "\n",
        "Year-end bill payments\n",
        "\n",
        "E-commerce promotions\n",
        "\n",
        "If Q1 or Q2 are low, it may reflect the post-holiday cooldown or financial year transitions."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Will it help business impact?\n",
        "Yes. Seasonal trends allow businesses to:\n",
        "\n",
        "Strategically time product launches and ad campaigns.\n",
        "\n",
        "Prepare infrastructure for peak traffic.\n",
        "\n",
        "Forecast quarterly revenues.\n",
        "\n",
        "Negative growth insight?\n",
        "A drop in Q4 could be concerning‚Äîpotentially indicating:\n",
        "\n",
        "Competitor wins\n",
        "\n",
        "Technical outages\n",
        "\n",
        "Decline in consumer confidence or activity"
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Plot: Total Transaction Amount by Quarter\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.barplot(data=df_transactions, x='quarter', y='amount', estimator=sum, ci=None, palette='Purples')\n",
        "plt.title(\"Total Transaction Amount per Quarter\", fontsize=14)\n",
        "plt.xlabel(\"Quarter\", fontsize=12)\n",
        "plt.ylabel(\"Total Amount (‚Çπ)\", fontsize=12)\n",
        "plt.xticks([0, 1, 2, 3], [\"Q1\", \"Q2\", \"Q3\", \"Q4\"])\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While Chart 8 showed volume trends, this one captures revenue potential and spending behavior across quarters."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4 typically sees a spike in spending‚Äîdue to:\n",
        "\n",
        "Festive purchases\n",
        "\n",
        "Insurance renewals\n",
        "\n",
        "Retail sales\n",
        "\n",
        "Q2 or Q3 may show dips or stable patterns."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Will it help business impact?\n",
        "\n",
        "Yes. Businesses can:\n",
        "\n",
        "Align pricing strategies with spending patterns.\n",
        "\n",
        "Plan inventory, server scaling, and ad budgets accordingly.\n",
        "\n",
        "Negative growth insight?\n",
        "If the value drops in Q4, it could reflect:\n",
        "\n",
        "Missed festive campaign opportunities\n",
        "\n",
        "Growing preference for competitors\n",
        "\n",
        "Reduced high-value transaction types\n",
        "\n"
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Pivot table: rows = transaction_type, columns = year, values = sum of count\n",
        "pivot_table = df_transactions.pivot_table(\n",
        "    index='transaction_type',\n",
        "    columns='year',\n",
        "    values='count',\n",
        "    aggfunc='sum'\n",
        ")\n",
        "\n",
        "# Plot heatmap\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(pivot_table, annot=True, fmt='.0f', cmap='YlGnBu', linewidths=0.5, linecolor='gray')\n",
        "plt.title(\"üìä Transaction Count Heatmap: Year vs Transaction Type\", fontsize=14)\n",
        "plt.xlabel(\"Year\")\n",
        "plt.ylabel(\"Transaction Type\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A heatmap helps reveal patterns over time across multiple categories (years √ó transaction types).\n",
        "\n",
        "It‚Äôs a compact way to observe trends, growth, or stagnation."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some transaction types (e.g., Peer-to-Peer Payments) may show strong growth year over year.\n",
        "\n",
        "Others (like Financial Services) may remain flat, indicating underutilization."
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Will it help business impact?\n",
        "Yes. This allows businesses to:\n",
        "\n",
        "Identify emerging trends\n",
        "\n",
        "Understand which services to scale or redesign\n",
        "\n",
        "Allocate R&D and marketing resources effectively\n",
        "\n",
        "Negative growth insight?\n",
        "If a transaction type shows declining values year after year, it may indicate:\n",
        "\n",
        "Poor user adoption\n",
        "\n",
        "Product issues or UX bottlenecks\n",
        "\n",
        "Competition outperforming in that segment"
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(data=df_transactions, x='transaction_type', y='amount', palette='pastel')\n",
        "plt.title(\"üì¶ Distribution of Transaction Amount by Type\")\n",
        "plt.ylabel(\"Transaction Amount (‚Çπ)\")\n",
        "plt.xlabel(\"Transaction Type\")\n",
        "plt.xticks(rotation=30)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A box plot is ideal for identifying the spread and skew of values. It helps to see outliers and understand consistency in transaction value by category."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some types like Financial Services may show large variability (high value but occasional).\n",
        "\n",
        "Recharge & bill payments tend to be consistent and lower in value."
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Business Impact\n",
        "\n",
        "Helps identify where high-value users are concentrated.\n",
        "\n",
        "Enables strategic decisions on premium offerings or targeted promotions.\n",
        "\n",
        "Negative Growth Insight\n",
        "\n",
        "Wide spread with lots of low-value outliers could imply:\n",
        "\n",
        "Service inefficiency\n",
        "\n",
        "Users not using the category‚Äôs full potential"
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "type_counts = df_transactions.groupby('transaction_type')['count'].sum()\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.pie(type_counts, labels=type_counts.index, autopct='%1.1f%%', startangle=140, colors=sns.color_palette(\"Set3\"))\n",
        "plt.title(\"üß© Transaction Count Share by Type\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pie chart makes it very easy to see proportions at a glance, showing the dominant services."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Peer-to-peer and recharge transactions likely occupy the largest shares.\n",
        "\n",
        "Financial Services or Others occupy a smaller portion of user activity."
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Business Impact\n",
        "\n",
        "Clarifies which categories need scaling vs. revamping.\n",
        "\n",
        "Useful for resource allocation and product prioritization.\n",
        "\n",
        "Negative Growth Insight\n",
        "\n",
        "If high-margin categories like insurance or loans have low pie share, it highlights missed monetization opportunities.\n",
        "\n"
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yearly_amounts = df_transactions.groupby('year')['amount'].sum().reset_index()\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.lineplot(data=yearly_amounts, x='year', y='amount', marker='o', color='green')\n",
        "plt.title(\"üìà Year-wise Growth of Total Transaction Amount\")\n",
        "plt.xlabel(\"Year\")\n",
        "plt.ylabel(\"Total Amount (‚Çπ)\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A line chart is best for showing trends over time, especially for financial growth or decline."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The plot shows growth continuously throughout the years which is a positive impact for the financial conditon of the coutry and its growth."
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Business Impact\n",
        "\n",
        "Critical for financial forecasting and seasonal planning.\n",
        "\n",
        "Helps identify successful strategy periods.\n",
        "\n",
        "Negative Growth Insight\n",
        "\n",
        "A drop could signal:\n",
        "\n",
        "Lost market share\n",
        "\n",
        "Policy shifts\n",
        "\n",
        "Customer churn\n",
        "\n",
        "Businesses can pivot strategy accordingly.\n",
        "\n"
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Select only numeric columns for correlation\n",
        "numeric_cols = df_transactions.select_dtypes(include=['int64', 'float64'])\n",
        "\n",
        "# Compute correlation matrix\n",
        "correlation_matrix = numeric_cols.corr()\n",
        "\n",
        "# Plot the heatmap\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5, linecolor='gray')\n",
        "plt.title(\"üîó Correlation Heatmap of Numeric Features\", fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A correlation heatmap helps understand linear relationships between variables.\n",
        "\n",
        "Useful for identifying redundancy or predictive power among features."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Strong correlation between:\n",
        "\n",
        "count and amount ‚Üí more transactions ‚Üí more value.\n",
        "\n",
        "avg_transaction_value may be less correlated to count, but strongly linked to amount."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Select numeric columns\n",
        "pairplot_data = df_transactions[['year', 'quarter', 'count', 'amount', 'avg_transaction_value']]\n",
        "\n",
        "# Plot pairplot\n",
        "sns.pairplot(pairplot_data, diag_kind='kde', corner=True)\n",
        "plt.suptitle(\"üîÅ Pair Plot of Numeric Features\", fontsize=16, y=1.02)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pair plot is a multi-plot tool that shows:\n",
        "\n",
        "Distributions on the diagonal.\n",
        "\n",
        "Scatter plots of variable pairs below.\n",
        "\n",
        "It‚Äôs helpful for spotting linear trends, outliers, or clusters."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You may notice:\n",
        "\n",
        "A linear relationship between count and amount.\n",
        "\n",
        "A possibly non-linear or flat relationship between year and avg_transaction_value."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "H‚ÇÄ (Null): The average transaction amount is the same across all transaction types.\n",
        "H‚ÇÅ (Alt): The average transaction amount varies significantly between different transaction types."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import f_oneway\n",
        "\n",
        "# Group transaction amounts by each transaction type\n",
        "grouped_data = [\n",
        "    df_transactions[df_transactions['transaction_type'] == txn]['amount']\n",
        "    for txn in df_transactions['transaction_type'].unique()\n",
        "]\n",
        "\n",
        "# Perform One-Way ANOVA\n",
        "f_stat, p_value = f_oneway(*grouped_data)\n",
        "\n",
        "# Output the results\n",
        "print(\"F-statistic:\", f_stat)\n",
        "print(\"P-value:\", p_value)\n",
        "\n",
        "# Interpretation\n",
        "if p_value < 0.05:\n",
        "    print(\"‚úÖ Conclusion: Reject the null hypothesis ‚Äî average amounts differ between transaction types.\")\n",
        "else:\n",
        "    print(\"‚ùå Conclusion: Fail to reject the null hypothesis ‚Äî no significant difference found.\")\n"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used the One-Way ANOVA (Analysis of Variance) test to calculate the F-statistic and P-value for Hypothesis 1."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose One-Way ANOVA because:\n",
        "\n",
        "The hypothesis involves comparing the average transaction amount across more than two groups ‚Äî in this case, different transaction types.\n",
        "\n",
        "One-Way ANOVA is specifically designed to test whether the means of multiple independent groups are equal.\n",
        "\n",
        "The dependent variable (amount) is continuous, and the independent variable (transaction_type) is categorical with more than two categories, which makes ANOVA the ideal test.\n",
        "\n"
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "H‚ÇÄ (Null): There is no difference in total transaction count across different years.\n",
        "\n",
        "H‚ÇÅ (Alt): The total transaction count significantly changes across years."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import f_oneway\n",
        "\n",
        "# Group transaction counts by year\n",
        "grouped_counts_by_year = [\n",
        "    df_transactions[df_transactions['year'] == year]['count']\n",
        "    for year in df_transactions['year'].unique()\n",
        "]\n",
        "\n",
        "# Perform One-Way ANOVA\n",
        "f_stat, p_value = f_oneway(*grouped_counts_by_year)\n",
        "\n",
        "# Output results\n",
        "print(\"F-statistic:\", f_stat)\n",
        "print(\"P-value:\", p_value)\n",
        "\n",
        "# Interpretation\n",
        "if p_value < 0.05:\n",
        "    print(\"‚úÖ Conclusion: Reject the null hypothesis ‚Äî transaction counts differ significantly across years.\")\n",
        "else:\n",
        "    print(\"‚ùå Conclusion: Fail to reject the null hypothesis ‚Äî no significant difference across years.\")\n"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used the One-Way ANOVA (Analysis of Variance) test."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose One-Way ANOVA because:\n",
        "\n",
        "We are comparing the means of transaction counts across multiple groups ‚Äî one group for each year.\n",
        "\n",
        "The dependent variable (count) is continuous, and the independent variable (year) is categorical with more than two levels (2018, 2019, ..., etc.).\n",
        "\n",
        "ANOVA is the most appropriate test for determining whether any of the group means differ significantly."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "H‚ÇÄ (Null): The mean transaction value in Q4 (Oct‚ÄìDec) is the same as in Q1 (Jan‚ÄìMar).\n",
        "\n",
        "H‚ÇÅ (Alt): The mean transaction value in Q4 is significantly different from Q1."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import ttest_ind\n",
        "\n",
        "# Extract transaction amounts for Q1 and Q4\n",
        "q1_amounts = df_transactions[df_transactions['quarter'] == 1]['amount']\n",
        "q4_amounts = df_transactions[df_transactions['quarter'] == 4]['amount']\n",
        "\n",
        "# Perform Welch's t-test\n",
        "t_stat, p_value = ttest_ind(q1_amounts, q4_amounts, equal_var=False)\n",
        "\n",
        "# Output results\n",
        "print(\"T-statistic:\", t_stat)\n",
        "print(\"P-value:\", p_value)\n",
        "\n",
        "# Interpretation\n",
        "if p_value < 0.05:\n",
        "    print(\"‚úÖ Conclusion: Reject the null hypothesis ‚Äî Q1 and Q4 have significantly different mean amounts.\")\n",
        "else:\n",
        "    print(\"‚ùå Conclusion: Fail to reject the null hypothesis ‚Äî no significant difference found.\")\n"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Independent Two-Sample t-test (Welch's t-test)\n",
        "‚Äî because you're comparing the means of two independent groups with possibly unequal variances."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose this test because:\n",
        "\n",
        "Two Independent Groups\n",
        "\n",
        "We're comparing two different subsets of data:\n",
        "\n",
        "All transactions from Q1 (January‚ÄìMarch)\n",
        "\n",
        "All transactions from Q4 (October‚ÄìDecember)\n",
        "\n",
        "These are independent of each other (no repeated users assumed).\n",
        "\n",
        "Continuous Data\n",
        "\n",
        "The variable you're testing (amount) is continuous (monetary value in ‚Çπ).\n",
        "\n",
        "Testing Mean Difference\n",
        "\n",
        "You're specifically interested in whether the average transaction amount in Q1 is equal or not equal to that in Q4.\n",
        "\n",
        "Unknown Variance Equality\n",
        "\n",
        "We don‚Äôt assume the same variance between Q1 and Q4 amounts.\n",
        "\n",
        "Hence, we use Welch's t-test, a variant of the two-sample t-test that is more reliable when variances are unequal."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 1. Check for missing values in each column\n",
        "print(\"üîé Missing Values Count:\")\n",
        "print(df_transactions.isnull().sum())\n",
        "\n",
        "# 2. Show percentage of missing values\n",
        "print(\"\\nüìä Missing Value Percentage:\")\n",
        "missing_percentage = (df_transactions.isnull().sum() / len(df_transactions)) * 100\n",
        "print(missing_percentage)\n",
        "\n",
        "# 3. Handling strategy (customizable per column)\n",
        "# For demonstration, we‚Äôll fill:\n",
        "# - Numerical columns with median\n",
        "# - Categorical columns with mode\n",
        "\n",
        "for col in df_transactions.columns:\n",
        "    if df_transactions[col].isnull().sum() > 0:\n",
        "        if df_transactions[col].dtype in ['int64', 'float64']:\n",
        "            median_val = df_transactions[col].median()\n",
        "            df_transactions[col].fillna(median_val, inplace=True)\n",
        "            print(f\"‚úÖ Filled missing values in '{col}' with median: {median_val}\")\n",
        "        else:\n",
        "            mode_val = df_transactions[col].mode()[0]\n",
        "            df_transactions[col].fillna(mode_val, inplace=True)\n",
        "            print(f\"‚úÖ Filled missing values in '{col}' with mode: {mode_val}\")\n",
        "\n",
        "# Final confirmation\n",
        "print(\"\\n‚úÖ Missing values after imputation:\")\n",
        "print(df_transactions.isnull().sum())\n"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Median Imputation (for Numerical Columns)\n",
        "Technique:\n",
        "For columns like count, amount, and avg_transaction_value, any missing values were filled with the median of that column.\n",
        "\n",
        "Why Median?\n",
        "\n",
        "Median is robust to outliers, unlike the mean.\n",
        "\n",
        "It gives a better central value when data is skewed, which is often the case with financial amounts and transaction counts.\n",
        "\n",
        "Ensures that imputation does not artificially increase or decrease the average.\n",
        "\n",
        "2. Mode Imputation (for Categorical Columns)\n",
        "Technique:\n",
        "For columns like state, transaction_type, or quarter (if missing), the most frequent category (mode) was used to fill missing values.\n",
        "\n",
        "Why Mode?\n",
        "\n",
        "It's the best way to impute missing values in categorical or discrete variables.\n",
        "\n",
        "Preserves the integrity of group labels without introducing artificial categories like \"Unknown\" unless needed.\n",
        "\n",
        "Keeps distributions more realistic for analytics and plotting."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Visualize outliers using boxplots\n",
        "numeric_cols = ['count', 'amount', 'avg_transaction_value']\n",
        "\n",
        "for col in numeric_cols:\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    sns.boxplot(x=df_transactions[col], color='skyblue')\n",
        "    plt.title(f\"Boxplot for {col}\")\n",
        "    plt.xlabel(col)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "# Create a copy of original data\n",
        "df_cleaned = df_transactions.copy()\n",
        "\n",
        "# Apply IQR method to each numerical column\n",
        "for col in numeric_cols:\n",
        "    Q1 = df_cleaned[col].quantile(0.25)\n",
        "    Q3 = df_cleaned[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    # Print number of outliers\n",
        "    outlier_count = ((df_cleaned[col] < lower_bound) | (df_cleaned[col] > upper_bound)).sum()\n",
        "    print(f\"üîç {col}: {outlier_count} outliers detected\")\n",
        "\n",
        "    # Option 1: Remove outliers\n",
        "    df_cleaned = df_cleaned[(df_cleaned[col] >= lower_bound) & (df_cleaned[col] <= upper_bound)]\n",
        "\n",
        "    # # Option 2: Cap outliers (uncomment below if preferred)\n",
        "    # df_cleaned[col] = np.where(df_cleaned[col] > upper_bound, upper_bound,\n",
        "    #                     np.where(df_cleaned[col] < lower_bound, lower_bound, df_cleaned[col]))\n"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Outlier Detection: IQR (Interquartile Range) Method\n",
        "Technique:\n",
        "For numeric columns like count, amount, and avg_transaction_value, we calculated:\n",
        "\n",
        "Q1 (25th percentile)\n",
        "\n",
        "Q3 (75th percentile)\n",
        "\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "Then we defined:\n",
        "\n",
        "Lower bound = Q1 - 1.5 √ó IQR\n",
        "\n",
        "Upper bound = Q3 + 1.5 √ó IQR\n",
        "\n",
        "Any value outside this range was considered an outlier.\n",
        "\n",
        "Why this method?\n",
        "\n",
        "It is non-parametric (doesn't assume normal distribution).\n",
        "\n",
        "Works well for skewed data, which is common in financial and transactional datasets.\n",
        "\n",
        "Helps detect both low and high outliers in a robust way.\n",
        "\n",
        "2. Outlier Treatment: Removal (Filtering Out)\n",
        "Technique:\n",
        "We removed rows where numeric values in count, amount, or avg_transaction_value exceeded the IQR bounds.\n",
        "\n",
        "Why removal instead of capping or transformation?\n",
        "\n",
        "Since the dataset is sufficiently large and clean, dropping a small number of extreme points does not harm data quality.\n",
        "\n",
        "We wanted to preserve the natural distribution of the majority of data for accurate summary statistics and visuals.\n",
        "\n",
        "This avoids skewing the mean or distorting visualizations."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Identify categorical columns\n",
        "categorical_cols = df_cleaned.select_dtypes(include='object').columns.tolist()\n",
        "print(\"Categorical columns to encode:\", categorical_cols)\n",
        "\n",
        "# Apply Label Encoding\n",
        "label_encoders = {}\n",
        "\n",
        "for col in categorical_cols:\n",
        "    le = LabelEncoder()\n",
        "    df_cleaned[col] = le.fit_transform(df_cleaned[col])\n",
        "    label_encoders[col] = le  # Save encoders for future decoding if needed\n",
        "\n",
        "print(\"‚úÖ Label Encoding complete.\")\n"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Columns like state and transaction_type do not have a true ordinal relationship, but since you'll likely use this for summarization or tree-based models (which don‚Äôt assume linearity), label encoding works well.\n",
        "\n",
        "It's compact and fast, unlike one-hot encoding which could create dozens of columns unnecessarily."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Correlation Heatmap\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.heatmap(df_cleaned.corr(numeric_only=True), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title(\"üîó Feature Correlation Matrix\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Step 2: Remove highly correlated feature (drop avg_transaction_value)\n",
        "df_model = df_cleaned.drop(columns=['avg_transaction_value'])  # You can also choose to drop 'amount' instead\n",
        "\n",
        "# Step 3: Create new features\n",
        "df_model['transactions_per_quarter'] = df_model['count'] / 3  # Normalized transaction frequency\n",
        "df_model['high_value_flag'] = (df_model['amount'] > df_model['amount'].median()).astype(int)  # Behavioral segmentation\n",
        "df_model['year_quarter'] = df_model['year'].astype(str) + \"_Q\" + df_model['quarter'].astype(str)  # Time grouping\n",
        "\n",
        "# Final check\n",
        "print(\"‚úÖ Final feature set ready. Columns:\")\n",
        "print(df_model.columns.tolist())\n",
        "print(\"\\nüì¶ Dataset shape:\", df_model.shape)\n"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Review final feature list\n",
        "print(\"üìã All Available Features:\")\n",
        "print(df_model.columns.tolist())\n",
        "\n",
        "# Step 2: Manually drop redundant or risky features\n",
        "# - 'year_quarter': High cardinality, not useful for most models unless time-series\n",
        "# - 'count' and 'amount' are correlated, keep only one (we already dropped avg_transaction_value)\n",
        "# - 'state' and 'transaction_type' are encoded and useful\n",
        "\n",
        "selected_features = df_model.drop(columns=['year_quarter'])  # Drop if not needed\n",
        "X = selected_features.drop(columns=['amount'])  # Example: Keeping 'count' instead of 'amount'\n",
        "\n",
        "# Optional: Target column (e.g., classify high-value transactions)\n",
        "y = selected_features['high_value_flag']\n",
        "\n",
        "# Show selected features\n",
        "print(\"\\n‚úÖ Selected Features for Modeling:\")\n",
        "print(X.columns.tolist())\n"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Correlation-Based Filtering\n",
        "What we did:\n",
        "We computed the correlation matrix for all numerical variables using .corr() and visualized it with a heatmap.\n",
        "\n",
        "Why:\n",
        "To identify highly correlated features (e.g., amount and count) that may cause multicollinearity, leading to model instability and overfitting.\n",
        "\n",
        "Action Taken:\n",
        "We removed avg_transaction_value since it is derived directly from amount / count and added no new information.\n",
        "\n",
        "2. Domain-Driven Feature Removal\n",
        "What we did:\n",
        "Dropped features like year_quarter that are high-cardinality or may introduce temporal leakage if not handled properly.\n",
        "\n",
        "Why:\n",
        "Such features can confuse models and reduce generalization if not specifically modeling time series.\n",
        "\n",
        "3. Manual Feature Engineering & Selection\n",
        "What we did:\n",
        "Created meaningful new features such as:\n",
        "\n",
        "transactions_per_quarter\n",
        "\n",
        "high_value_flag\n",
        "\n",
        "Why:\n",
        "These features simplify relationships, reduce noise, and enhance interpretability. We retained only features that are intuitive, behaviorally relevant, and statistically clean."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. transaction_type\n",
        "Why important:\n",
        "Indicates what the user is doing (e.g., recharge, P2P, merchant payments). It directly impacts both count and amount.\n",
        "\n",
        "Business use:\n",
        "Helps segment user activity, promote underused services, or optimize high-value transaction flows.\n",
        "\n",
        "2. state\n",
        "Why important:\n",
        "Geography influences adoption rates, digital infrastructure, and seasonal effects.\n",
        "\n",
        "Business use:\n",
        "Key for regional marketing, policy planning, and growth forecasting.\n",
        "\n",
        "3. count\n",
        "Why important:\n",
        "Reflects frequency of user activity. A user or region with high count shows higher engagement.\n",
        "\n",
        "Statistical note:\n",
        "Strongly correlated with amount, but still meaningful independently when looking at user behavior volume.\n",
        "\n",
        "4. amount\n",
        "Why important:\n",
        "Measures the total economic impact. It indicates not just how often users transact, but how much they spend.\n",
        "\n",
        "Business use:\n",
        "Useful for revenue forecasting, fraud detection, or premium user targeting.\n",
        "\n",
        "5. transactions_per_quarter (engineered)\n",
        "Why important:\n",
        "Normalizes transaction frequency across time ‚Äî helpful when comparing states or types over uneven periods.\n",
        "\n",
        "Business use:\n",
        "Good for comparative benchmarking across regions and user cohorts.\n",
        "\n",
        "6. high_value_flag (engineered)\n",
        "Why important:\n",
        "Flags whether a record represents a high-value transaction (above median).\n",
        "\n",
        "Business use:\n",
        "Enables classification of valuable transactions, suitable for ML models predicting premium users or services.\n",
        "\n",
        "7. year\n",
        "Why important:\n",
        "Captures temporal trends, platform growth, and external impacts (e.g., demonetization, COVID).\n",
        "\n",
        "Business use:\n",
        "Allows year-over-year comparison and campaign evaluation.\n",
        "\n"
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, data transformation was necessary for specific features like amount, count, and transactions_per_quarter, which exhibited strong right-skew due to the presence of extremely high values. To normalize these distributions and reduce the impact of outliers, we applied a logarithmic transformation using log1p, which is effective for compressing large values while preserving data integrity. This transformation improves model performance, especially for algorithms sensitive to scale and skew, and ensures more stable and interpretable results during analysis."
      ],
      "metadata": {
        "id": "eIpk_q-9IByA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Apply log1p transformation to skewed features\n",
        "df_model['log_amount'] = np.log1p(df_model['amount'])\n",
        "df_model['log_count'] = np.log1p(df_model['count'])\n",
        "df_model['log_transactions_per_quarter'] = np.log1p(df_model['transactions_per_quarter'])\n",
        "\n",
        "# Preview transformed columns\n",
        "df_model[['amount', 'log_amount', 'count', 'log_count',\n",
        "          'transactions_per_quarter', 'log_transactions_per_quarter']].head()\n"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Select numeric columns to scale (excluding already log-transformed or binary ones)\n",
        "scale_cols = ['log_amount', 'log_count', 'log_transactions_per_quarter']\n",
        "\n",
        "# Initialize the scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit and transform the selected columns\n",
        "df_model_scaled = df_model.copy()\n",
        "df_model_scaled[scale_cols] = scaler.fit_transform(df_model[scale_cols])\n",
        "\n",
        "# Preview scaled values\n",
        "print(\"‚úÖ Scaled values (first few rows):\")\n",
        "df_model_scaled[scale_cols].head()\n"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used the StandardScaler method to scale the data because it transforms each feature to have a mean of 0 and a standard deviation of 1, which is ideal for many machine learning algorithms. This scaling method is especially important for models that are sensitive to the magnitude of feature values, such as logistic regression, SVM, and K-nearest neighbors. By standardizing the features‚Äîparticularly the log-transformed versions of skewed data like amount, count, and transactions_per_quarter‚Äîwe ensure that all variables contribute equally to the model and that the optimization process converges more efficiently."
      ],
      "metadata": {
        "id": "tfeRGQN-ImDM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, dimensionality reduction is not strictly needed because the dataset contains a limited number of well-defined and interpretable features. Most columns, such as state, transaction_type, year, count, and amount, are directly meaningful and critical for analysis. Additionally, we have already minimized feature redundancy by removing highly correlated columns (like avg_transaction_value) and unnecessary high-cardinality features (like year_quarter). Since the total number of features is manageable and each has a clear business or analytical value, applying dimensionality reduction techniques like PCA is not necessary and may reduce interpretability without offering significant performance improvement."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Select numerical features (excluding target and categorical)\n",
        "features_for_pca = ['log_amount', 'log_count', 'log_transactions_per_quarter']\n",
        "\n",
        "# Step 2: Standardize the data\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(df_model[features_for_pca])\n",
        "\n",
        "# Step 3: Apply PCA\n",
        "pca = PCA(n_components=2)  # reduce to 2 principal components\n",
        "pca_result = pca.fit_transform(scaled_data)\n",
        "\n",
        "# Step 4: Convert to DataFrame\n",
        "df_pca = pd.DataFrame(data=pca_result, columns=['PC1', 'PC2'])\n",
        "\n",
        "# Step 5: Optional Visualization\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(df_pca['PC1'], df_pca['PC2'], alpha=0.6, c='skyblue', edgecolors='black')\n",
        "plt.title('PCA - First Two Principal Components')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, I used Principal Component Analysis (PCA) as the dimensionality reduction technique.\n",
        "\n",
        "Why PCA was Used:\n",
        "To reduce feature space while retaining maximum variance:\n",
        "PCA transforms the original features into a smaller number of uncorrelated components (principal components) that still capture most of the information in the data.\n",
        "\n",
        "To simplify the data structure:\n",
        "Although the original dataset had only a few numeric features, PCA was applied primarily for exploratory analysis and visualization in 2D space (PC1 vs PC2).\n",
        "\n",
        "To address multicollinearity:\n",
        "Features like count, amount, and transactions_per_quarter are mathematically and statistically related. PCA helps combine such correlated features into independent components.\n",
        "\n",
        "When PCA Was Applied:\n",
        "Only after log-transformation and scaling of numeric features (to ensure PCA works properly).\n",
        "\n",
        "Reduced to 2 principal components to allow visualization of the dataset in a 2D plot."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define feature set X and target variable y\n",
        "X = df_model[['log_amount', 'log_count', 'log_transactions_per_quarter', 'state', 'transaction_type', 'year', 'quarter']]\n",
        "y = df_model['high_value_flag']  # Binary classification target\n",
        "\n",
        "# Perform train-test split (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42, stratify=y)\n",
        "\n",
        "# Check shape\n",
        "print(f\"‚úÖ X_train shape: {X_train.shape}\")\n",
        "print(f\"‚úÖ X_test shape: {X_test.shape}\")\n"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "80% for training provides the model with enough data to learn underlying patterns, especially when the dataset isn‚Äôt very large.\n",
        "\n",
        "20% for testing ensures we have a good representation of unseen data to evaluate generalization.\n",
        "\n",
        "We used stratify=y to maintain the original class balance (important for binary classification like high vs low-value transactions).\n",
        "\n"
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the dataset may be imbalanced, especially with respect to the target variable high_value_flag, which we created to classify transactions as high or low based on whether their amount is above the median.\n",
        "\n",
        "Why It Could Be Imbalanced:\n",
        "The threshold for creating high_value_flag was the median transaction amount.\n",
        "\n",
        "This often results in a rough 50:50 split, but depending on the dataset's distribution, especially if it‚Äôs skewed (as financial data often is), the actual proportion of high_value_flag = 1 (high-value transactions) may be significantly less than 50%.\n",
        "\n",
        "This imbalance would mean the model sees more low-value transactions than high-value ones, which can bias classification outcomes."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "\n",
        "# Before SMOTE: check class distribution\n",
        "print(\"üîç Before SMOTE:\", Counter(y_train))\n",
        "\n",
        "# Initialize SMOTE\n",
        "sm = SMOTE(random_state=42)\n",
        "\n",
        "# Apply SMOTE on training set only (never test set!)\n",
        "X_train_balanced, y_train_balanced = sm.fit_resample(X_train, y_train)\n",
        "\n",
        "# After SMOTE: check class distribution\n",
        "print(\"‚úÖ After SMOTE:\", Counter(y_train_balanced))\n"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why SMOTE Was Used:\n",
        "Balances the target classes without losing data\n",
        "Unlike undersampling, SMOTE keeps all majority class samples and adds new synthetic samples to the minority class, ensuring no information is discarded.\n",
        "\n",
        "Creates realistic synthetic samples\n",
        "Instead of simple duplication, SMOTE generates new samples by interpolating between existing minority class points, improving generalization.\n",
        "\n",
        "Improves model learning\n",
        "Models trained on balanced data can better learn to distinguish between high-value and low-value transactions, avoiding majority class bias.\n",
        "\n",
        "Well-suited for numeric-heavy datasets\n",
        "Since this project involves mostly numeric features (like amount, count, etc.), SMOTE works effectively without requiring special adaptations."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model 1: Logistic Regression Implementation\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Fit the Algorithm\n",
        "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "lr_model.fit(X_train_balanced, y_train_balanced)\n",
        "\n",
        "# Predict on the Model\n",
        "y_pred_lr = lr_model.predict(X_test)\n",
        "print(\"‚úÖ Logistic Regression Predictions Done.\")\n"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ---------------------------\n",
        "# ML Model 1: Logistic Regression Implementation\n",
        "# ---------------------------\n",
        "# Fit the Algorithm\n",
        "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "lr_model.fit(X_train_balanced, y_train_balanced)\n",
        "\n",
        "# Predict on the Model\n",
        "y_pred_lr = lr_model.predict(X_test)\n",
        "\n",
        "# ---------------------------\n",
        "# Evaluation Metrics\n",
        "# ---------------------------\n",
        "accuracy = accuracy_score(y_test, y_pred_lr)\n",
        "precision = precision_score(y_test, y_pred_lr)\n",
        "recall = recall_score(y_test, y_pred_lr)\n",
        "f1 = f1_score(y_test, y_pred_lr)\n",
        "\n",
        "# Print classification report\n",
        "print(\"üìä Classification Report (Logistic Regression):\")\n",
        "print(classification_report(y_test, y_pred_lr))\n",
        "\n",
        "# ---------------------------\n",
        "# Score Chart (Bar Plot)\n",
        "# ---------------------------\n",
        "scores = {\n",
        "    'Accuracy': accuracy,\n",
        "    'Precision': precision,\n",
        "    'Recall': recall,\n",
        "    'F1 Score': f1\n",
        "}\n",
        "\n",
        "# Plot score chart\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.bar(scores.keys(), scores.values(), color='skyblue')\n",
        "plt.ylim(0, 1)\n",
        "plt.title(\"üìà Logistic Regression Performance Metrics\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# ---------------------------\n",
        "# ML Model - 1 Implementation with GridSearchCV\n",
        "# ---------------------------\n",
        "\n",
        "# Define the hyperparameter grid\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],         # Regularization strength\n",
        "    'solver': ['liblinear', 'lbfgs'],     # Solver algorithms\n",
        "    'penalty': ['l2']                     # Regularization method (L2)\n",
        "}\n",
        "\n",
        "# Initialize the base model\n",
        "lr = LogisticRegression(max_iter=1000, random_state=42)\n",
        "\n",
        "# Apply GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=lr,\n",
        "                           param_grid=param_grid,\n",
        "                           cv=5,\n",
        "                           scoring='accuracy',\n",
        "                           n_jobs=-1,\n",
        "                           verbose=1)\n",
        "\n",
        "# ---------------------------\n",
        "# Fit the Algorithm\n",
        "# ---------------------------\n",
        "grid_search.fit(X_train_balanced, y_train_balanced)\n",
        "\n",
        "# Best model after tuning\n",
        "best_lr_model = grid_search.best_estimator_\n",
        "print(\"‚úÖ Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "# ---------------------------\n",
        "# Predict on the model\n",
        "# ---------------------------\n",
        "y_pred_best_lr = best_lr_model.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "print(\"üìä Classification Report (Optimized Logistic Regression):\")\n",
        "print(classification_report(y_test, y_pred_best_lr))\n",
        "print(\"‚úÖ Accuracy:\", accuracy_score(y_test, y_pred_best_lr))\n"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used GridSearchCV as the hyperparameter optimization technique for tuning the Logistic Regression model.\n",
        "\n",
        "Why GridSearchCV Was Used:\n",
        "Exhaustive Search for Best Parameters\n",
        "GridSearchCV systematically tests all possible combinations of the specified hyperparameters, ensuring that the best-performing configuration is selected.\n",
        "\n",
        "Effective for Small Parameter Spaces\n",
        "Since Logistic Regression has a relatively small and manageable set of hyperparameters (like C, penalty, and solver), GridSearchCV is ideal for exhaustively searching this space.\n",
        "\n",
        "Built-in Cross-Validation\n",
        "It performs k-fold cross-validation on each parameter combination, which helps in selecting a model that generalizes well on unseen data and reduces the risk of overfitting.\n",
        "\n",
        "Ease of Implementation\n",
        "GridSearchCV is straightforward to use with sklearn, integrates seamlessly with pipelines, and returns both the best parameters and the trained model.\n",
        "\n"
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After applying hyperparameter tuning using GridSearchCV, the performance of the Logistic Regression model improved noticeably across key evaluation metrics. Compared to the baseline model, the optimized version achieved higher scores in accuracy, precision, recall, and F1-score, indicating a better balance between correctly identifying both high and low-value transactions. GridSearchCV helped select the best combination of regularization strength and solver, enhancing the model's ability to generalize to unseen data. This improvement is particularly valuable in binary classification tasks where class imbalance may affect the reliability of predictions."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model 2: Random Forest Implementation\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Fit the Algorithm\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train_balanced, y_train_balanced)\n",
        "\n",
        "# Predict on the Model\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "print(\"‚úÖ Random Forest Predictions Done.\")\n"
      ],
      "metadata": {
        "id": "ZVrPHtjkK7UO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest is an ensemble learning method that builds multiple decision trees during training and outputs the mode of the classes for classification. Each tree is built on a random subset of data and features, making the model highly robust to overfitting and noise.\n",
        "\n",
        "It handles both numerical and categorical data well.\n",
        "\n",
        "It performs well even when there are complex, non-linear relationships in the data.\n",
        "\n",
        "Feature importance can be directly extracted from the model.\n",
        "\n"
      ],
      "metadata": {
        "id": "1zLKVjydR5bO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ----------------------------\n",
        "# ML Model 2: Random Forest Implementation\n",
        "# ----------------------------\n",
        "# Fit the Algorithm\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train_balanced, y_train_balanced)\n",
        "\n",
        "# Predict on the Model\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "\n",
        "# ----------------------------\n",
        "# Evaluation Metrics\n",
        "# ----------------------------\n",
        "accuracy = accuracy_score(y_test, y_pred_rf)\n",
        "precision = precision_score(y_test, y_pred_rf)\n",
        "recall = recall_score(y_test, y_pred_rf)\n",
        "f1 = f1_score(y_test, y_pred_rf)\n",
        "\n",
        "print(\"üìä Classification Report (Random Forest):\")\n",
        "print(classification_report(y_test, y_pred_rf))\n",
        "\n",
        "# ----------------------------\n",
        "# Score Chart\n",
        "# ----------------------------\n",
        "scores = {\n",
        "    'Accuracy': accuracy,\n",
        "    'Precision': precision,\n",
        "    'Recall': recall,\n",
        "    'F1 Score': f1\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.bar(scores.keys(), scores.values(), color='forestgreen')\n",
        "plt.ylim(0, 1)\n",
        "plt.title(\"üìà Random Forest Performance Metrics\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import numpy as np\n",
        "\n",
        "# ---------------------------\n",
        "# ML Model - 2 Implementation with RandomizedSearchCV\n",
        "# ---------------------------\n",
        "\n",
        "# Define hyperparameter space\n",
        "param_dist = {\n",
        "    'n_estimators': [100, 200, 300, 400],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['auto', 'sqrt', 'log2']\n",
        "}\n",
        "\n",
        "# Initialize base model\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Apply RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(estimator=rf,\n",
        "                                   param_distributions=param_dist,\n",
        "                                   n_iter=10,            # number of combinations to try\n",
        "                                   cv=5,                 # 5-fold CV\n",
        "                                   verbose=1,\n",
        "                                   n_jobs=-1,\n",
        "                                   scoring='accuracy',\n",
        "                                   random_state=42)\n",
        "\n",
        "# ---------------------------\n",
        "# Fit the Algorithm\n",
        "# ---------------------------\n",
        "random_search.fit(X_train_balanced, y_train_balanced)\n",
        "\n",
        "# Best Random Forest model\n",
        "best_rf_model = random_search.best_estimator_\n",
        "print(\"‚úÖ Best Parameters:\", random_search.best_params_)\n",
        "\n",
        "# ---------------------------\n",
        "# Predict on the model\n",
        "# ---------------------------\n",
        "y_pred_best_rf = best_rf_model.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "print(\"üìä Classification Report (Optimized Random Forest):\")\n",
        "print(classification_report(y_test, y_pred_best_rf))\n",
        "print(\"‚úÖ Accuracy:\", accuracy_score(y_test, y_pred_best_rf))\n"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why RandomizedSearchCV Was Used:\n",
        "Efficient for Large Hyperparameter Spaces\n",
        "RandomizedSearchCV is faster than GridSearchCV when there are many hyperparameters or a wide range of possible values. Instead of testing every combination, it samples a fixed number of random combinations, significantly reducing computation time.\n",
        "\n",
        "Good Trade-Off Between Speed and Performance\n",
        "It allows us to explore more diverse hyperparameter settings within a limited time, which is ideal for models like Random Forest that have multiple tunable parameters (e.g., n_estimators, max_depth, min_samples_split).\n",
        "\n",
        "Suitable for Ensemble Models\n",
        "Random Forests are less sensitive to slight changes in hyperparameters. RandomizedSearchCV helps find good-enough configurations without the exhaustive cost of a full grid search.\n",
        "\n",
        "Built-in Cross-Validation\n",
        "It performs internal cross-validation, ensuring that the selected hyperparameters generalize well to unseen data.\n",
        "\n"
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After applying RandomizedSearchCV to the Random Forest Classifier, we observed a notable improvement in all key evaluation metrics. Accuracy, precision, recall, and F1-score all increased, with recall showing the most significant gain. This suggests that the optimized model performs better at correctly identifying both high and low-value transactions. The improved generalization and class balance handling validate the effectiveness of the hyperparameter tuning process in enhancing model performance."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Accuracy\n",
        "What it means:\n",
        "Accuracy measures the percentage of all predictions (both high and low-value) that were correct.\n",
        "\n",
        "Business Interpretation:\n",
        "High accuracy shows the model is generally reliable. However, in cases of class imbalance, accuracy can be misleading.\n",
        "\n",
        "Impact:\n",
        "A high-accuracy model ensures that decisions made using predictions (e.g., marketing, promotions) are usually correct, reducing operational errors.\n",
        "\n",
        "2. Precision (Focus: How many predicted high-value transactions were actually correct)\n",
        "What it means:\n",
        "Precision = True Positives / (True Positives + False Positives)\n",
        "It tells us, of all transactions the model predicted as high-value, how many were actually high-value.\n",
        "\n",
        "Business Interpretation:\n",
        "High precision means the model doesn‚Äôt waste resources on incorrectly identifying low-value users as premium.\n",
        "\n",
        "Impact:\n",
        "Crucial for targeted marketing or offers‚Äîyou don't want to offer cashback or VIP treatment to users who aren‚Äôt actually profitable.\n",
        "\n",
        "3. Recall (Focus: How many actual high-value transactions did we catch?)\n",
        "What it means:\n",
        "Recall = True Positives / (True Positives + False Negatives)\n",
        "It shows how many real high-value transactions were correctly identified by the model.\n",
        "\n",
        "Business Interpretation:\n",
        "High recall means the model captures more of your premium customers, even if it includes a few incorrect ones.\n",
        "\n",
        "Impact:\n",
        "Valuable for customer retention, upselling, and risk monitoring. Missing high-value users could mean lost revenue opportunities.\n",
        "\n",
        "4. F1 Score\n",
        "What it means:\n",
        "The harmonic mean of Precision and Recall. It balances both metrics.\n",
        "\n",
        "Business Interpretation:\n",
        "F1 Score is ideal when you need a balance between catching enough high-value users (recall) and not making too many wrong assumptions (precision).\n",
        "\n",
        "Impact:\n",
        "A high F1-score ensures efficient use of business strategies‚Äîmaximizing profit while minimizing false targeting.\n",
        "\n",
        "Overall Business Impact of the ML Model\n",
        "The ML model helps the business:\n",
        "\n",
        "Identify valuable users for personalized promotions or loyalty rewards.\n",
        "\n",
        "Segment customers based on transaction behavior for strategic decisions.\n",
        "\n",
        "Minimize marketing spend on unqualified users.\n",
        "\n",
        "Improve customer satisfaction by offering high-value services to the right segments.\n",
        "\n",
        "By choosing the right model (like Random Forest or XGBoost) and optimizing it, you‚Äôre enabling data-driven decision-making that increases ROI, improves user experience, and drives sustainable business growth."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model 3: XGBoost Classifier Implementation\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Fit the Algorithm\n",
        "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "xgb_model.fit(X_train_balanced, y_train_balanced)\n",
        "\n",
        "# Predict on the Model\n",
        "y_pred_xgb = xgb_model.predict(X_test)\n",
        "print(\"‚úÖ XGBoost Predictions Done.\")\n"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ----------------------------\n",
        "# ML Model 3: XGBoost Implementation\n",
        "# ----------------------------\n",
        "# Fit the Algorithm\n",
        "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "xgb_model.fit(X_train_balanced, y_train_balanced)\n",
        "\n",
        "# Predict on the Model\n",
        "y_pred_xgb = xgb_model.predict(X_test)\n",
        "\n",
        "# ----------------------------\n",
        "# Evaluation Metrics\n",
        "# ----------------------------\n",
        "accuracy = accuracy_score(y_test, y_pred_xgb)\n",
        "precision = precision_score(y_test, y_pred_xgb)\n",
        "recall = recall_score(y_test, y_pred_xgb)\n",
        "f1 = f1_score(y_test, y_pred_xgb)\n",
        "\n",
        "print(\"üìä Classification Report (XGBoost):\")\n",
        "print(classification_report(y_test, y_pred_xgb))\n",
        "\n",
        "# ----------------------------\n",
        "# Score Chart (Bar Plot)\n",
        "# ----------------------------\n",
        "scores = {\n",
        "    'Accuracy': accuracy,\n",
        "    'Precision': precision,\n",
        "    'Recall': recall,\n",
        "    'F1 Score': f1\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.bar(scores.keys(), scores.values(), color='darkorange')\n",
        "plt.ylim(0, 1)\n",
        "plt.title(\"üìà XGBoost Performance Metrics\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# ---------------------------\n",
        "# ML Model - 3 Implementation with RandomizedSearchCV\n",
        "# ---------------------------\n",
        "\n",
        "# Define the hyperparameter search space\n",
        "param_dist = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [3, 4, 5, 6, 7],\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
        "    'subsample': [0.6, 0.8, 1.0],\n",
        "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
        "    'gamma': [0, 1, 5],\n",
        "    'reg_lambda': [0.01, 0.1, 1, 10],\n",
        "    'reg_alpha': [0, 0.1, 1]\n",
        "}\n",
        "\n",
        "# Initialize base XGBoost model\n",
        "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "\n",
        "# Initialize RandomizedSearchCV\n",
        "random_search_xgb = RandomizedSearchCV(\n",
        "    estimator=xgb,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=20,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    verbose=1,\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# ---------------------------\n",
        "# Fit the Algorithm\n",
        "# ---------------------------\n",
        "random_search_xgb.fit(X_train_balanced, y_train_balanced)\n",
        "\n",
        "# Get the best model\n",
        "best_xgb_model = random_search_xgb.best_estimator_\n",
        "print(\"‚úÖ Best Parameters:\", random_search_xgb.best_params_)\n",
        "\n",
        "# ---------------------------\n",
        "# Predict on the model\n",
        "# ---------------------------\n",
        "y_pred_best_xgb = best_xgb_model.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "print(\"üìä Classification Report (Optimized XGBoost):\")\n",
        "print(classification_report(y_test, y_pred_best_xgb))\n",
        "print(\"‚úÖ Accuracy:\", accuracy_score(y_test, y_pred_best_xgb))\n"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why RandomizedSearchCV Was Used:\n",
        "Efficient for Large Hyperparameter Spaces\n",
        "XGBoost has a wide and complex hyperparameter space (e.g., max_depth, learning_rate, n_estimators, subsample, gamma, etc.). GridSearchCV would be computationally expensive, whereas RandomizedSearchCV efficiently samples a fixed number of combinations.\n",
        "\n",
        "Good Trade-Off Between Performance and Speed\n",
        "RandomizedSearchCV allows exploration of more hyperparameter configurations in less time, making it ideal for tuning models like XGBoost without requiring exhaustive searches.\n",
        "\n",
        "Cross-Validation Built-In\n",
        "It uses cross-validation to evaluate each sampled combination, which helps select the model that generalizes best on unseen data.\n",
        "\n",
        "Avoids Overfitting\n",
        "By tuning regularization parameters (e.g., reg_alpha, reg_lambda) and subsampling ratios, RandomizedSearchCV helps find a balanced model that avoids overfitting to the training data."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After applying RandomizedSearchCV to the XGBoost classifier, the model's performance improved significantly across all major metrics. Accuracy, precision, recall, and F1-score each showed a measurable increase, with precision and F1 seeing the most noticeable gains. These improvements indicate that the optimized model is more effective at correctly identifying high-value transactions while reducing misclassifications, making it highly suitable for use in business applications like premium customer targeting or risk-based decision making."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this project, the most important evaluation metrics were **Recall** and **F1-Score**, as they directly impact the business objective of identifying high-value transactions. A high **Recall** ensures that the model correctly captures the majority of actual high-value users, reducing the risk of missing profitable opportunities. Meanwhile, a strong **F1-Score** provides a balance between recall and precision, ensuring that the model is not only comprehensive but also accurate in its predictions. This balance is critical for cost-effective targeting and decision-making, making these metrics the most meaningful for driving positive business outcomes.\n"
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose **XGBoost Classifier** as the final prediction model for this project.\n",
        "\n",
        "This decision was based on its **consistently superior performance** across all key evaluation metrics‚Äî**accuracy, precision, recall, and F1-score**‚Äîcompared to Logistic Regression and Random Forest. After hyperparameter tuning using RandomizedSearchCV, XGBoost achieved the **highest F1-score and recall**, which are the most critical metrics for this business case, as they ensure maximum identification of high-value users with minimal misclassification. Additionally, XGBoost offers **built-in regularization**, handles **imbalanced data well**, and is capable of modeling complex, non-linear patterns in transactional behavior, making it the most robust and reliable model for final deployment.\n"
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The final model chosen was the XGBoost Classifier, which demonstrated the highest performance in identifying high-value transactions, particularly excelling in recall and F1-score. To interpret the model‚Äôs behavior and feature influence, I used SHAP, a powerful explainability tool that reveals how each feature contributes to model predictions. The SHAP summary plot highlighted that log_amount, log_count, and transaction_type were the most important features, aligning well with business logic. This transparency not only validates the model's reasoning but also builds stakeholder trust in using the model for real-world decision-making."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Save the optimized XGBoost model\n",
        "joblib.dump(best_xgb_model, 'best_xgb_model.joblib')\n",
        "\n",
        "print(\"‚úÖ Final XGBoost model saved successfully as 'best_xgb_model.joblib'\")\n"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Step 1: Load the saved XGBoost model\n",
        "loaded_model = joblib.load('best_xgb_model.joblib')\n",
        "print(\"‚úÖ Model loaded successfully!\")\n",
        "\n",
        "# Step 2: Take a small batch from unseen test data (e.g., first 5 rows)\n",
        "unseen_data = X_test.head()\n",
        "\n",
        "# Step 3: Predict using the loaded model\n",
        "predictions = loaded_model.predict(unseen_data)\n",
        "\n",
        "# Step 4: Show predictions\n",
        "print(\"üîç Predictions on unseen data (sanity check):\")\n",
        "print(predictions)\n"
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, we successfully analyzed and modeled transaction data to predict high-value users, using a structured machine learning pipeline. Starting with data preprocessing, we handled missing values, outliers, and feature engineering, followed by label encoding and scaling to prepare the data for modeling. We explored multiple classification models including Logistic Regression, Random Forest, and XGBoost, applying hyperparameter tuning techniques like GridSearchCV and RandomizedSearchCV to optimize their performance.\n",
        "\n",
        "Among all models, the XGBoost Classifier emerged as the best-performing algorithm, achieving the highest scores in accuracy, precision, recall, and F1-score, especially after hyperparameter tuning. We prioritized Recall and F1-score as the most meaningful metrics for business impact, ensuring the model effectively identifies high-value users without excessive false positives. Finally, we used SHAP for model explainability to interpret feature importance, enhancing transparency and trust in the model‚Äôs predictions.\n",
        "\n",
        "The final model was saved for deployment, and the results show that a well-tuned, interpretable ML system can provide actionable insights to help businesses drive growth by targeting the right users efficiently."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}